{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "\n",
        "در این تمرین، یک مدل درخت تصمیم به‌صورت دستی برای داده‌های معروف Iris پیاده‌سازی شد. ابتدا داده‌ها نرمال‌سازی شدند و سپس با استفاده از تابعی بازگشتی، درخت تصمیم با معیار Gini ساخته شد. برای هر گره، بهترین ویژگی و مقدار آستانه به‌صورت بهینه انتخاب شد تا داده‌ها به صورت حداکثری از هم جدا شوند.\n",
        "\n",
        "بعد از ساخت مدل، آن را روی مجموعه تست ارزیابی کردیم و معیارهای مختلفی از جمله دقت، precision، recall و f1-score محاسبه شدند. تمام این مقادیر بالا بودند که نشان می‌دهد درخت تصمیم توانسته به‌خوبی ساختار داده را یاد بگیرد و تعمیم بدهد.\n",
        "\n",
        "ساختار نهایی درخت نیز ساده ولی مؤثر بود؛ به‌گونه‌ای که کلاس Setosa تنها با یک شرط از بقیه جدا شد و برای تفکیک Versicolor و Virginica از دو ویژگی دیگر با چند تقسیم اضافی استفاده شد. این نتایج نشان می‌دهد که درخت تصمیم با عمق محدود هم توانسته ساختار داده‌ها را به‌خوبی یاد بگیرد و دسته‌بندی دقیقی انجام دهد.\n",
        "\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "9dfQC4DswnFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "_cell_guid": "9e92f1a3-3e81-4ec6-a567-890827b0555c",
        "_uuid": "e5338e8c3fa6dc8f410d8b868aa78cb54621780b",
        "collapsed": true,
        "id": "0c8jQ7HSuiSO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "# from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "ما توی این تمرین باید یه SVM (ماشین بردار پشتیبان) رو از صفر پیاده‌سازی می‌کردیم تا دیتاست Iris رو دسته‌بندی کنیم. اول از همه اومدیم مسئله رو ساده کردیم و تبدیلش کردیم به یه مسئله دوتایی (باینری). یعنی فقط گونه‌ی Setosa رو با مقدار ۱ در نظر گرفتیم و بقیه‌ی گونه‌ها رو با ‎‑۱، چون توی حالت چندکلاسه کدنویسی سخت‌تر می‌شد. بعد رفتیم سراغ اینکه مدل SVM رو با استفاده از تابع hinge loss و گرادیان نزولی (gradient descent) آموزش بدیم.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "LDPfqtML6e_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bn_k4zMcuiSW"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = datasets.load_iris()\n",
        "X0 = data.data\n",
        "y0 = data.target\n",
        "y0 = np.where(y0 == 0, 1, -1)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X0, y0, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "این تابع برای افزایش ویژگی‌ها با کرنل چندجمله‌ای (Polynomial) هست. اگه poly رو True بزاریم، داده‌ها به درجه‌های بالاتر مثل x² و x³ هم گسترش پیدا می‌کنن تا مدل بتونه مرزهای غیرخطی یاد بگیره. ولی ما فعلاً خاموش گذاشتیم (poly_on = False) چون SVM با داده Setosa به‌خوبی خطی جدا می‌شه.\n",
        "</div>"
      ],
      "metadata": {
        "id": "BrbQhvDu61L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feat(x, poly=False, deg=2):\n",
        "    if not poly:\n",
        "        return x\n",
        "    z = x.copy()\n",
        "    for d in range(2, deg + 1):\n",
        "        z = np.hstack((z, x ** d))\n",
        "    return z\n"
      ],
      "metadata": {
        "id": "3xALQTSm6tzP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "این تابع hinge loss رو حساب می‌کنه که نشون می‌ده چقدر مدل از نظر فاصله گرفتن از نقاط اشتباه کار می‌کنه. اگر y * (w·x + b) بزرگتر مساوی ۱ باشه یعنی نمونه به‌خوبی جدا شده، ولی اگر کوچکتر باشه جریمه داره.\n",
        "</div>"
      ],
      "metadata": {
        "id": "tTnK8b2v7Fhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PaE5r0dW6uBC"
      },
      "outputs": [],
      "source": [
        "def hinge(X, y, w, b, lam):\n",
        "    m = X.shape[0]\n",
        "    margins = 1 - y * (X.dot(w) + b)\n",
        "    loss = np.maximum(0, margins).mean() + lam * np.dot(w, w)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "برای هر epoch (دور) و هر نمونه از داده، چک کردیم که شرط margin رعایت شده یا نه. اگه رعایت نشده بود وزن و بایاس رو با فرمول مخصوص به‌روزرسانی کردیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "9bYsVvKC7QLM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f8bdf9-4136-4378-c272-d8f0c3c7e07c",
        "id": "zDMQgVqE6w-g"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 1.0\n",
            "loss: 0.01163211724797394\n",
            "cm:\n",
            " [[10  0]\n",
            " [ 0 20]]\n",
            "prec: 1.0\n",
            "rec: 1.0\n",
            "f1: 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = datasets.load_iris()\n",
        "X0 = data.data\n",
        "y0 = data.target\n",
        "y0 = np.where(y0 == 0, 1, -1)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X0, y0, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "poly_on = False\n",
        "Xtr = feat(Xtr, poly_on, 2)\n",
        "Xte = feat(Xte, poly_on, 2)\n",
        "\n",
        "w = np.zeros(Xtr.shape[1])\n",
        "b = 0\n",
        "lr = 1e-3\n",
        "lam = 1e-2\n",
        "epoch = 1000\n",
        "\n",
        "\n",
        "\n",
        "# lr = \"0.01\"\n",
        "# w = np.ones(99)\n",
        "# for i in range(epoch): pass\n",
        "\n",
        "for _ in range(epoch):\n",
        "    for i, xi in enumerate(Xtr):\n",
        "        cond = ytr[i] * (np.dot(xi, w) + b)\n",
        "        if cond >= 1:\n",
        "            dw = 2 * lam * w\n",
        "            db = 0\n",
        "        else:\n",
        "            dw = 2 * lam * w - ytr[i] * xi\n",
        "            db = -ytr[i]\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "\n",
        "loss_val = hinge(Xte, yte, w, b, lam)\n",
        "\n",
        "def pred(X):\n",
        "    return np.sign(X.dot(w) + b)\n",
        "\n",
        "yp = pred(Xte)\n",
        "acc = (yp == yte).mean()\n",
        "cm = confusion_matrix(yte, yp, labels=[1, -1])\n",
        "pr = precision_score(yte, yp, pos_label=1, zero_division=0)\n",
        "rc = recall_score(yte, yp, pos_label=1, zero_division=0)\n",
        "f1 = f1_score(yte, yp, pos_label=1, zero_division=0)\n",
        "\n",
        "print(\"acc:\", acc)\n",
        "print(\"loss:\", loss_val)\n",
        "print(\"cm:\\n\", cm)\n",
        "print(\"prec:\", pr)\n",
        "print(\"rec:\", rc)\n",
        "print(\"f1:\", f1)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}